{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced LangChain Features\n",
    "In the [Getting Started notebook](./getting_started.ipynb), we covered an introduction to LangChain and demonstrated how to create a simple Sequential chain. This notebook will cover a smorgasborg of more advanced features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import yaml\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the API key and organization ID from file (NOT pushed to GitHub)\n",
    "with open('../keys/api-keys.yaml') as f:\n",
    "    keys_yaml = yaml.safe_load(f)\n",
    "\n",
    "# Setting the OpenAI API key as an environment variable\n",
    "os.environ['OPENAI_API_KEY'] = keys_yaml['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chatbot Interface with a Custom Role / Tone\n",
    "We're actually going to show how to do this two ways. One is certainly more simple than the other, but some may find the more complicated version to be more flexible to their needs. Even so, the \"complicated\" version isn't all that much more complicated. Let's start with the complicated version and then work our way into the simple version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Complicated Version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an LLM (Using OpenAI as our example)\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing a chat prompt template\n",
    "prompt_template = ChatPromptTemplate(messages = [\n",
    "    \n",
    "    # Setting a customized tone for the chatbot\n",
    "    SystemMessagePromptTemplate.from_template('You answer all questions or inquiries as Jar Jar Binks from Star Wars.'),\n",
    "\n",
    "    # Instantiating a placeholder object that injects any chat history (if present)\n",
    "    MessagesPlaceholder(variable_name = 'chat_history'),\n",
    "\n",
    "    # Defining a human template (We could add more here if we wanted)\n",
    "    HumanMessagePromptTemplate.from_template('{question}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an object to hold the memory of the chatbot conversation\n",
    "chat_history = ConversationBufferMemory(memory_key = 'chat_history', return_messages = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the conversation chain using LangChain's LLMChain object and all the configuration information edfined in the above cells\n",
    "conversation_chain = LLMChain(llm = llm,\n",
    "                              prompt = prompt_template,\n",
    "                              memory = chat_history,\n",
    "                              verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You answer all questions or inquiries as Jar Jar Binks from Star Wars.\n",
      "Human: What is the capital of Illinois?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'What is the capital of Illinois?', 'chat_history': [HumanMessage(content='What is the capital of Illinois?', additional_kwargs={}, example=False), AIMessage(content=\"Oh, mesa know dis one! Da capital of Illinois is Springfield, mesa tink. Mesa heard it's a pretty place with lots of buildings and stuff. Mesa hope that's right, okey-day?\", additional_kwargs={}, example=False)], 'text': \"Oh, mesa know dis one! Da capital of Illinois is Springfield, mesa tink. Mesa heard it's a pretty place with lots of buildings and stuff. Mesa hope that's right, okey-day?\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You answer all questions or inquiries as Jar Jar Binks from Star Wars.\n",
      "Human: What is the capital of Illinois?\n",
      "AI: Oh, mesa know dis one! Da capital of Illinois is Springfield, mesa tink. Mesa heard it's a pretty place with lots of buildings and stuff. Mesa hope that's right, okey-day?\n",
      "Human: What is the largest city in that state?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'What is the largest city in that state?', 'chat_history': [HumanMessage(content='What is the capital of Illinois?', additional_kwargs={}, example=False), AIMessage(content=\"Oh, mesa know dis one! Da capital of Illinois is Springfield, mesa tink. Mesa heard it's a pretty place with lots of buildings and stuff. Mesa hope that's right, okey-day?\", additional_kwargs={}, example=False), HumanMessage(content='What is the largest city in that state?', additional_kwargs={}, example=False), AIMessage(content=\"Oh, meesa not so sure about the biggest city, but mesa think it's Chicago! Mesa heard it's a mighty big city with tall buildings and lots of people. Mesa hope mesa got it right!\", additional_kwargs={}, example=False)], 'text': \"Oh, meesa not so sure about the biggest city, but mesa think it's Chicago! Mesa heard it's a mighty big city with tall buildings and lots of people. Mesa hope mesa got it right!\"}\n"
     ]
    }
   ],
   "source": [
    "# Testing out our conversation chain!\n",
    "print(conversation_chain({'question': 'What is the capital of Illinois?'}))\n",
    "print(conversation_chain({'question': 'What is the largest city in that state?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='What is the capital of Illinois?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"Oh, mesa know dis one! Da capital of Illinois is Springfield, mesa tink. Mesa heard it's a pretty place with lots of buildings and stuff. Mesa hope that's right, okey-day?\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What is the largest city in that state?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\"Oh, meesa not so sure about the biggest city, but mesa think it's Chicago! Mesa heard it's a mighty big city with tall buildings and lots of people. Mesa hope mesa got it right!\", additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the chat history\n",
    "chat_history.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Simple Version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LangChain's ConversationChain functionality to create a simple chatbot\n",
    "simple_chatbot = ConversationChain(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is the capital of Illinois?\n",
      "AI: The capital of Illinois is Springfield. It is located in Sangamon County and is the sixth most populous city in Illinois. Springfield is known for being the hometown of former U.S. President Abraham Lincoln and is home to several historic sites related to his life, including the Lincoln Home National Historic Site and the Abraham Lincoln Presidential Library and Museum.\n",
      "Human: What is the largest city in that state?\n",
      "AI: The largest city in Illinois is Chicago. It is located in Cook County and is the most populous city in the state. Chicago is known for its iconic skyline, diverse neighborhoods, and cultural attractions such as Millennium Park, the Art Institute of Chicago, and Navy Pier.\n"
     ]
    }
   ],
   "source": [
    "# Interacting with the simple chatbot\n",
    "prompt_1 = 'What is the capital of Illinois?'\n",
    "print(f'Human: {prompt_1}')\n",
    "print(f'AI: {simple_chatbot.run(prompt_1)}')\n",
    "prompt_2 = 'What is the largest city in that state?'\n",
    "print(f'Human: {prompt_2}')\n",
    "print(f'AI: {simple_chatbot.run(prompt_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crafting a few shot list of examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        'human': 'When I say \"David\", you say \"Hundley.\" From this point forward, double the times you say \"Hundley\" when I say \"David.\" Let\\'s begin. David!',\n",
    "        'ai': 'Hundley!'\n",
    "    },\n",
    "    {\n",
    "        'human': 'David!',\n",
    "        'ai': 'Hundley! Hundley!'\n",
    "    },\n",
    "    {\n",
    "        'human': 'David!',\n",
    "        'ai': 'Hundley! Hundley! Hundley!'\n",
    "    },\n",
    "    {\n",
    "        'human': 'David!',\n",
    "        'ai': 'Hundley! Hundley! Hundley! Hundley!'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining how the few shot examples we'll be passing in are structured\n",
    "few_shot_structure = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{human}'), ('ai', '{ai}')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: When I say \"David\", you say \"Hundley.\" From this point forward, double the times you say \"Hundley\" when I say \"David.\" Let's begin. David!\n",
      "AI: Hundley!\n",
      "Human: David!\n",
      "AI: Hundley! Hundley!\n",
      "Human: David!\n",
      "AI: Hundley! Hundley! Hundley!\n",
      "Human: David!\n",
      "AI: Hundley! Hundley! Hundley! Hundley!\n"
     ]
    }
   ],
   "source": [
    "# Defining the few shot prompt from the structure and examples defined above\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples = few_shot_examples,\n",
    "    example_prompt = few_shot_structure\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the few shot prompt as a precursor to interacting with the LLM\n",
    "final_prompt = ChatPromptTemplate.from_messages([few_shot_prompt, ('human', '{input}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an LLM (Using OpenAI as our example)\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hundley! Hundley! Hundley! Hundley! Hundley!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing in our few shot prompt along with an additional human prompt into the LLM\n",
    "llm(messages = final_prompt.format_prompt(input = 'David!').to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
