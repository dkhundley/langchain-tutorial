{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain\n",
    "This notebook shows how to assemble a basic LangChain chain of tasks using the OpenAI API as the backend. This tutorial will require you to have your own OpenAI API key."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import yaml\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the API key and organization ID from file (NOT pushed to GitHub)\n",
    "with open('../keys/openai-keys.yaml') as f:\n",
    "    keys_yaml = yaml.safe_load(f)\n",
    "\n",
    "# Setting the OpenAI API key as an environment variable\n",
    "os.environ['OPENAI_API_KEY'] = keys_yaml['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LangChain to connect to OpenAI\n",
    "openai_llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an object to obtain results from the Wikipedia API\n",
    "wikipedia_api = WikipediaAPIWrapper(top_k_results = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "In order to create a chained set of tasks, we need to instantiate the individual tasks with **prompt templates**. Prompt templates combine an input alongside some preset feature engineering to produce a precise result (also known as a Completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a prompt template for checking to see if the inputted individual is a historical figure\n",
    "is_historical_figure_template = PromptTemplate(\n",
    "    input_variables = ['entity_name'],\n",
    "    template = 'Is the following entity a person, and if yes, would you consider this person to be a historical figure: {entity_name}. Please give me back just a yes or no answer.'\n",
    ")\n",
    "\n",
    "# Creating a prompt template for generating a research paper outline the three most important events in the historical figure's life\n",
    "research_paper_outline_template = PromptTemplate(\n",
    "    input_variables = ['entity_name', 'wikipedia_entry'],\n",
    "    template = 'The following is a Wikipedia entry about {entity_name}. Please provide for me an outline of a basic research paper with an introduction, the three most important events of this person\\'s life, and a conclusion. Here is the Wikipedia information:\\n\\n {wikipedia_entry}'\n",
    ")\n",
    "\n",
    "# Creating a prompt template for generating a research paper based on the outline generated from the previous prompt template\n",
    "research_paper_template = PromptTemplate(\n",
    "    input_variables = ['entity_name', 'research_paper_outline'],\n",
    "    template = 'You are a high schooler who has just been assigned the task of writing a short research paper about a historical figure. The historical figure is {entity_name}, and the following is an outline to follow:\\n {research_paper_outline}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating objects to hold the intermediate memory of each LangChain step\n",
    "research_paper_outline_memory = ConversationBufferMemory(input_key = 'research_paper_outline_prompt', memory_key = 'chat_history')\n",
    "research_paper_memory = ConversationBufferMemory(input_key = 'research_paper_prompt', memory_key = 'chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the LangChains objects for all the prompt templates defined above\n",
    "is_historical_figure_chain = LLMChain(llm = openai_llm, prompt = is_historical_figure_template, verbose = True, output_key = 'is_historical_figure')\n",
    "research_paper_outline_chain = LLMChain(llm = openai_llm, prompt = research_paper_outline_template, verbose = True, output_key = 'research_paper_outline', memory = research_paper_outline_memory)\n",
    "research_paper_chain = LLMChain(llm = openai_llm, prompt = research_paper_template, verbose = True, output_key = 'research_paper', memory = research_paper_memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LangChain chain\n",
    "research_paper_langchain = SequentialChain(chains = [research_paper_outline_chain, research_paper_chain],\n",
    "                                           input_variables = ['entity_name', 'wikipedia_entry'],\n",
    "                                           output_variables = ['research_paper'],\n",
    "                                           verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
